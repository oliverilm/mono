{"version":3,"file":"IHttpLlmApplication.mjs","sources":["../../src/structures/IHttpLlmApplication.ts"],"sourcesContent":["import { OpenApi } from \"../OpenApi\";\nimport { IHttpLlmFunction } from \"./IHttpLlmFunction\";\nimport { IHttpMigrateRoute } from \"./IHttpMigrateRoute\";\nimport { ILlmApplication } from \"./ILlmApplication\";\nimport { ILlmSchema } from \"./ILlmSchema\";\nimport { ILlmSchemaV3 } from \"./ILlmSchemaV3\";\n\n/**\n * Application of LLM function call from OpenAPI document.\n *\n * `IHttpLlmApplication` is a data structure representing a collection of\n * {@link IHttpLlmFunction LLM function calling schemas} composed from the\n * {@link OpenApi.IDocument OpenAPI document} and its {@link OpenApi.IOperation operation}\n * metadata. It also contains {@link IHttpLlmApplication.errors failed operations}, and\n * adjusted {@link IHttpLlmApplication.options options} during the `IHttpLlmApplication`\n * construction.\n *\n * About the {@link OpenApi.IOperation API operations}, they are converted to\n * {@link IHttpLlmFunction} type which represents LLM function calling schema.\n * By the way, if there're some types which does not supported by LLM, the operation\n * would be failed and pushed into the {@link IHttpLlmApplication.errors}. Otherwise not,\n * the operation would be successfully converted to {@link IHttpLlmFunction} and its\n * type schemas are downgraded to {@link OpenApiV3.IJsonSchema} and converted to\n * {@link ILlmSchemaV3}.\n *\n * About the options, if you've configured {@link IHttpLlmApplication.options.keyword}\n * (as `true`), number of {@link IHttpLlmFunction.parameters} are always 1 and the first\n * parameter type is always {@link ILlmSchemaV3.IObject}. Otherwise, the parameters would\n * be multiple, and the sequence of the parameters are following below rules.\n *\n * - `pathParameters`: Path parameters of {@link IHttpMigrateRoute.parameters}\n * - `query`: Query parameter of {@link IHttpMigrateRoute.query}\n * - `body`: Body parameter of {@link IHttpMigrateRoute.body}\n *\n * ```typescript\n * // KEYWORD TRUE\n * {\n *   ...pathParameters,\n *   query,\n *   body,\n * }\n *\n * // KEYWORD FALSE\n * [\n *   ...pathParameters,\n *   ...(query ? [query] : []),\n *   ...(body ? [body] : []),\n * ]\n * ```\n *\n * By the way, there can be some parameters (or their nested properties) which must be\n * composed by Human, not by LLM. File uploading feature or some sensitive information\n * like secrety key (password) are the examples. In that case, you can separate the\n * function parameters to both LLM and Human sides by configuring the\n * {@link IHttpLlmApplication.IOptions.separate} property. The separated parameters are\n * assigned to the {@link IHttpLlmFunction.separated} property.\n *\n * For reference, the actual function call execution is not by LLM, but by you.\n * When the LLM selects the proper function and fills the arguments, you just call\n * the function by {@link HttpLlm.execute} with the LLM prepared arguments. And then\n * informs the return value to the LLM by system prompt. The LLM will continue the next\n * conversation based on the return value.\n *\n * Additionally, if you've configured {@link IHttpLlmApplication.IOptions.separate},\n * so that the parameters are separated to Human and LLM sides, you can merge these\n * humand and LLM sides' parameters into one through {@link HttpLlm.mergeParameters}\n * before the actual LLM function call execution.\n *\n * @author Jeongho Nam - https://github.com/samchon\n */\nexport interface IHttpLlmApplication<Model extends ILlmSchema.Model> {\n  /**\n   * Model of the target LLM.\n   */\n  model: Model;\n\n  /**\n   * List of function metadata.\n   *\n   * List of function metadata that can be used for the LLM function call.\n   *\n   * When you want to execute the function with LLM constructed arguments,\n   * you can do it through {@link LlmFetcher.execute} function.\n   */\n  functions: IHttpLlmFunction<Model>[];\n\n  /**\n   * List of errors occurred during the composition.\n   */\n  errors: IHttpLlmApplication.IError[];\n\n  /**\n   * Configuration for the application.\n   */\n  options: IHttpLlmApplication.IOptions<Model>;\n}\nexport namespace IHttpLlmApplication {\n  export import IOptions = ILlmApplication.IOptions;\n\n  /**\n   * Error occurred in the composition.\n   */\n  export interface IError {\n    /**\n     * HTTP method of the endpoint.\n     */\n    method: \"get\" | \"post\" | \"put\" | \"patch\" | \"delete\" | \"head\";\n\n    /**\n     * Path of the endpoint.\n     */\n    path: string;\n\n    /**\n     * Error messsages.\n     */\n    messages: string[];\n\n    /**\n     * Get the Swagger operation metadata.\n     *\n     * Get the Swagger operation metadata, of the source.\n     */\n    operation: () => OpenApi.IOperation;\n\n    /**\n     * Get the migration route metadata.\n     *\n     * Get the migration route metadata, of the source.\n     *\n     * If the property returns `undefined`, it means that the error has\n     * been occured in the migration level, not of LLM application composition.\n     *\n     * @returns Migration route metadata.\n     */\n    route: () => IHttpMigrateRoute | undefined;\n  }\n}\n"],"names":["IHttpLlmApplication"],"mappings":"AAgGM,IAAWA;;CAAjB,SAAiBA,sBAyChB,EAzCD,CAAiBA,wBAAAA,sBAyChB,CAAA;;"}